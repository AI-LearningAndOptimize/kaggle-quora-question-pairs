{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_use_gpus(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list_id = 'oofp_siamese_lstm_attention_magic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = load(aux_data_folder + 'embedding_weights_fasttext_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_q1 = load(features_data_folder + 'X_train_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_train_q2 = load(features_data_folder + 'X_train_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_q1 = load(features_data_folder + 'X_test_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_test_q2 = load(features_data_folder + 'X_test_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "magic_feature_lists = [\n",
    "    'magic_jturkewitz',\n",
    "    'magic_stas_svd_150',\n",
    "    'magic_stas_avito',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_magic, X_test_magic, _ = load_feature_lists(magic_feature_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_magic = X_train_magic.values\n",
    "X_test_magic = X_test_magic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack([X_train_magic, X_test_magic]))\n",
    "X_train_magic = scaler.transform(X_train_magic)\n",
    "X_test_magic = scaler.transform(X_test_magic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = load(features_data_folder + 'y_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[-1]\n",
    "VOCAB_LENGTH = embedding_matrix.shape[0]\n",
    "MAX_SEQUENCE_LENGTH = X_train_q1.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 101442 30\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDING_DIM, VOCAB_LENGTH, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models & Compute Out-of-Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, init='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None,  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight((input_shape[-1], 1),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.b = self.add_weight((input_shape[1],),\n",
    "                                 initializer='zero',\n",
    "                                 name='{}_b'.format(self.name),\n",
    "                                 regularizer=self.bias_regularizer,\n",
    "                                 constraint=self.bias_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[1],),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, mask):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # (x, 40, 300) x (300, 1)\n",
    "        multData =  K.dot(x, self.kernel) # (x, 40, 1)\n",
    "        multData = K.squeeze(multData, -1) # (x, 40)\n",
    "        multData = multData + self.b # (x, 40) + (40,)\n",
    "\n",
    "        multData = K.tanh(multData) # (x, 40)\n",
    "\n",
    "        multData = multData * self.u # (x, 40) * (40, 1) => (x, 1)\n",
    "        multData = K.exp(multData) # (X, 1)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx()) #(x, 40)\n",
    "            multData = mask*multData #(x, 40) * (x, 40, )\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        multData /= K.cast(K.sum(multData, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        multData = K.expand_dims(multData)\n",
    "        weighted_input = x * multData\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dense_block(input_layer, num_units, dropout_rate):\n",
    "    dense = Dense(num_units)(input_layer)\n",
    "    bn = BatchNormalization()(dense)\n",
    "    relu = Activation('relu')(bn)\n",
    "    dropout = Dropout(dropout_rate)(relu)\n",
    "    output = dropout\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    embedding_layer = Embedding(\n",
    "        VOCAB_LENGTH,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,\n",
    "    )\n",
    "    lstm_layer = LSTM(\n",
    "        params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "        return_sequences=True,\n",
    "    )\n",
    "    attention_layer = AttentionWithContext()\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = attention_layer(lstm_layer(embedded_sequences_1))\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = attention_layer(lstm_layer(embedded_sequences_2))\n",
    "\n",
    "    magic_input = Input(shape=(X_train_magic.shape[-1], ))\n",
    "    \n",
    "    merged = concatenate([x1, y1, magic_input])\n",
    "    dropout = Dropout(params['dense_dropout_rate'])(merged)\n",
    "    \n",
    "    dense_1 = create_dense_block(dropout, params['num_dense_1'], params['dense_dropout_rate'])\n",
    "    dense_2 = create_dense_block(dropout, params['num_dense_2'], params['dense_dropout_rate'])\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(dense_2)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[sequence_1_input, sequence_2_input, magic_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint_path = aux_data_folder + 'fold-checkpoint-' + feature_list_id + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_oofp = np.zeros_like(y_train, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_oofp = np.zeros((len(X_test_q1), NUM_FOLDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'dense_dropout_rate': 0.164,\n",
    "    'lstm_dropout_rate': 0.324,\n",
    "    'num_dense_1': 128,\n",
    "    'num_dense_2': 64,\n",
    "    'num_lstm': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X_q1, X_q2, X_magic):\n",
    "    y1 = model.predict(\n",
    "        [X_q1, X_q2, X_magic],\n",
    "        batch_size=1024,\n",
    "        verbose=1\n",
    "    ).reshape(-1)\n",
    "    \n",
    "    y2 = model.predict(\n",
    "        [X_q2, X_q1, X_magic],\n",
    "        batch_size=1024,\n",
    "        verbose=1\n",
    "    ).reshape(-1)\n",
    "    \n",
    "    return (y1 + y2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting fold 1 of 5\n",
      "\n",
      "Train on 646862 samples, validate on 161718 samples\n",
      "Epoch 1/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8323Epoch 00000: val_loss improved from inf to 0.32349, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 506s - loss: 0.3784 - acc: 0.8323 - val_loss: 0.3235 - val_acc: 0.8575\n",
      "Epoch 2/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8444Epoch 00001: val_loss improved from 0.32349 to 0.30919, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 508s - loss: 0.3447 - acc: 0.8444 - val_loss: 0.3092 - val_acc: 0.8628\n",
      "Epoch 3/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.8505Epoch 00002: val_loss improved from 0.30919 to 0.30613, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 506s - loss: 0.3303 - acc: 0.8505 - val_loss: 0.3061 - val_acc: 0.8657\n",
      "Epoch 4/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8547Epoch 00003: val_loss improved from 0.30613 to 0.29718, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 510s - loss: 0.3216 - acc: 0.8547 - val_loss: 0.2972 - val_acc: 0.8674\n",
      "Epoch 5/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3160 - acc: 0.8571- ETA: 0s - loss: 0.3159 - acc: 0.85Epoch 00004: val_loss improved from 0.29718 to 0.29507, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 510s - loss: 0.3160 - acc: 0.8571 - val_loss: 0.2951 - val_acc: 0.8690\n",
      "Epoch 6/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8589Epoch 00005: val_loss improved from 0.29507 to 0.29160, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 510s - loss: 0.3114 - acc: 0.8589 - val_loss: 0.2916 - val_acc: 0.8706\n",
      "Epoch 7/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.8611Epoch 00006: val_loss improved from 0.29160 to 0.28964, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 507s - loss: 0.3074 - acc: 0.8611 - val_loss: 0.2896 - val_acc: 0.8711\n",
      "Epoch 8/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.8622Epoch 00007: val_loss did not improve\n",
      "646862/646862 [==============================] - 509s - loss: 0.3047 - acc: 0.8622 - val_loss: 0.2907 - val_acc: 0.8706\n",
      "Epoch 9/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.8633Epoch 00008: val_loss improved from 0.28964 to 0.28961, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 509s - loss: 0.3028 - acc: 0.8633 - val_loss: 0.2896 - val_acc: 0.8714\n",
      "Epoch 10/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.8646Epoch 00009: val_loss improved from 0.28961 to 0.28926, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 510s - loss: 0.3006 - acc: 0.8646 - val_loss: 0.2893 - val_acc: 0.8707\n",
      "Epoch 11/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.8654Epoch 00010: val_loss improved from 0.28926 to 0.28708, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 508s - loss: 0.2983 - acc: 0.8654 - val_loss: 0.2871 - val_acc: 0.8718\n",
      "Epoch 12/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.8657Epoch 00011: val_loss improved from 0.28708 to 0.28624, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 510s - loss: 0.2976 - acc: 0.8657 - val_loss: 0.2862 - val_acc: 0.8733\n",
      "Epoch 13/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.8669- ETA: 0s - loss: 0.2956 - acc: 0.86Epoch 00012: val_loss did not improve\n",
      "646862/646862 [==============================] - 506s - loss: 0.2956 - acc: 0.8669 - val_loss: 0.2876 - val_acc: 0.8703\n",
      "Epoch 14/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.8642Epoch 00013: val_loss did not improve\n",
      "646862/646862 [==============================] - 510s - loss: 0.3010 - acc: 0.8642 - val_loss: 0.2879 - val_acc: 0.8715\n",
      "Epoch 15/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.8647Epoch 00014: val_loss did not improve\n",
      "646862/646862 [==============================] - 510s - loss: 0.3007 - acc: 0.8647 - val_loss: 0.2885 - val_acc: 0.8701\n",
      "Epoch 00014: early stopping\n",
      "80859/80859 [==============================] - 3s     \n",
      "80859/80859 [==============================] - 3s     \n",
      "2345796/2345796 [==============================] - 107s   \n",
      "\n",
      "Fitting fold 2 of 5\n",
      "\n",
      "Train on 646862 samples, validate on 161718 samples\n",
      "Epoch 1/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8335Epoch 00000: val_loss improved from inf to 0.32955, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 500s - loss: 0.3762 - acc: 0.8335 - val_loss: 0.3296 - val_acc: 0.8549\n",
      "Epoch 2/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8457Epoch 00001: val_loss improved from 0.32955 to 0.31076, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 504s - loss: 0.3414 - acc: 0.8457 - val_loss: 0.3108 - val_acc: 0.8622\n",
      "Epoch 3/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8519Epoch 00002: val_loss improved from 0.31076 to 0.30050, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 504s - loss: 0.3264 - acc: 0.8519 - val_loss: 0.3005 - val_acc: 0.8669\n",
      "Epoch 4/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8554Epoch 00003: val_loss improved from 0.30050 to 0.29766, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 502s - loss: 0.3184 - acc: 0.8554 - val_loss: 0.2977 - val_acc: 0.8684\n",
      "Epoch 5/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8577Epoch 00004: val_loss improved from 0.29766 to 0.29359, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 493s - loss: 0.3136 - acc: 0.8577 - val_loss: 0.2936 - val_acc: 0.8694\n",
      "Epoch 6/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8604Epoch 00005: val_loss improved from 0.29359 to 0.28773, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646862/646862 [==============================] - 483s - loss: 0.3078 - acc: 0.8604 - val_loss: 0.2877 - val_acc: 0.8720\n",
      "Epoch 7/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.8617Epoch 00006: val_loss did not improve\n",
      "646862/646862 [==============================] - 442s - loss: 0.3047 - acc: 0.8617 - val_loss: 0.2889 - val_acc: 0.8732\n",
      "Epoch 8/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.8636Epoch 00007: val_loss improved from 0.28773 to 0.28443, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646862/646862 [==============================] - 439s - loss: 0.3015 - acc: 0.8636 - val_loss: 0.2844 - val_acc: 0.8742\n",
      "Epoch 9/200\n",
      "646784/646862 [============================>.] - ETA: 0s - loss: 0.2991 - acc: 0.8647Epoch 00008: val_loss did not improve\n",
      "646862/646862 [==============================] - 439s - loss: 0.2991 - acc: 0.8647 - val_loss: 0.2849 - val_acc: 0.8742\n",
      "Epoch 10/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.8651Epoch 00009: val_loss did not improve\n",
      "646862/646862 [==============================] - 439s - loss: 0.2974 - acc: 0.8651 - val_loss: 0.2858 - val_acc: 0.8728\n",
      "Epoch 11/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.8661Epoch 00010: val_loss did not improve\n",
      "646862/646862 [==============================] - 439s - loss: 0.2957 - acc: 0.8661 - val_loss: 0.2853 - val_acc: 0.8741\n",
      "Epoch 12/200\n",
      "646848/646862 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.8676Epoch 00011: val_loss did not improve\n",
      "646862/646862 [==============================] - 440s - loss: 0.2938 - acc: 0.8676 - val_loss: 0.2853 - val_acc: 0.8725\n",
      "Epoch 00011: early stopping\n",
      "80859/80859 [==============================] - 3s     \n",
      "80859/80859 [==============================] - 3s     \n",
      "2345796/2345796 [==============================] - 106s   \n",
      "\n",
      "Fitting fold 3 of 5\n",
      "\n",
      "Train on 646864 samples, validate on 161716 samples\n",
      "Epoch 1/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8332Epoch 00000: val_loss improved from inf to 0.32596, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3778 - acc: 0.8332 - val_loss: 0.3260 - val_acc: 0.8535\n",
      "Epoch 2/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8448Epoch 00001: val_loss improved from 0.32596 to 0.30703, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3433 - acc: 0.8448 - val_loss: 0.3070 - val_acc: 0.8628\n",
      "Epoch 3/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8508Epoch 00002: val_loss improved from 0.30703 to 0.30326, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3291 - acc: 0.8508 - val_loss: 0.3033 - val_acc: 0.8662\n",
      "Epoch 4/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.8548Epoch 00003: val_loss improved from 0.30326 to 0.30054, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3209 - acc: 0.8548 - val_loss: 0.3005 - val_acc: 0.8656\n",
      "Epoch 5/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.8573Epoch 00004: val_loss improved from 0.30054 to 0.29829, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3152 - acc: 0.8573 - val_loss: 0.2983 - val_acc: 0.8660\n",
      "Epoch 6/200\n",
      "646848/646864 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8594Epoch 00005: val_loss improved from 0.29829 to 0.29394, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3120 - acc: 0.8594 - val_loss: 0.2939 - val_acc: 0.8680\n",
      "Epoch 7/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.8611Epoch 00006: val_loss improved from 0.29394 to 0.29162, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 442s - loss: 0.3076 - acc: 0.8611 - val_loss: 0.2916 - val_acc: 0.8707\n",
      "Epoch 8/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.8623Epoch 00007: val_loss improved from 0.29162 to 0.28948, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646864/646864 [==============================] - 443s - loss: 0.3041 - acc: 0.8623 - val_loss: 0.2895 - val_acc: 0.8715\n",
      "Epoch 9/200\n",
      "646848/646864 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.8633Epoch 00008: val_loss did not improve\n",
      "646864/646864 [==============================] - 444s - loss: 0.3024 - acc: 0.8633 - val_loss: 0.2897 - val_acc: 0.8701\n",
      "Epoch 10/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.8642Epoch 00009: val_loss did not improve\n",
      "646864/646864 [==============================] - 444s - loss: 0.3018 - acc: 0.8642 - val_loss: 0.2932 - val_acc: 0.8670\n",
      "Epoch 11/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.8652Epoch 00010: val_loss did not improve\n",
      "646864/646864 [==============================] - 443s - loss: 0.2986 - acc: 0.8652 - val_loss: 0.2901 - val_acc: 0.8702\n",
      "Epoch 12/200\n",
      "646784/646864 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8523Epoch 00011: val_loss did not improve\n",
      "646864/646864 [==============================] - 443s - loss: 0.3294 - acc: 0.8523 - val_loss: 0.3018 - val_acc: 0.8655\n",
      "Epoch 00011: early stopping\n",
      "80858/80858 [==============================] - 3s     \n",
      "80858/80858 [==============================] - 3s     \n",
      "2345796/2345796 [==============================] - 107s   \n",
      "\n",
      "Fitting fold 4 of 5\n",
      "\n",
      "Train on 646866 samples, validate on 161714 samples\n",
      "Epoch 1/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8327Epoch 00000: val_loss improved from inf to 0.32162, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.3776 - acc: 0.8327 - val_loss: 0.3216 - val_acc: 0.8578\n",
      "Epoch 2/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8448Epoch 00001: val_loss improved from 0.32162 to 0.30737, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.3440 - acc: 0.8448 - val_loss: 0.3074 - val_acc: 0.8633\n",
      "Epoch 3/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8504Epoch 00002: val_loss improved from 0.30737 to 0.30405, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.3306 - acc: 0.8504 - val_loss: 0.3040 - val_acc: 0.8636\n",
      "Epoch 4/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8540Epoch 00003: val_loss improved from 0.30405 to 0.29777, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646866/646866 [==============================] - 440s - loss: 0.3222 - acc: 0.8540 - val_loss: 0.2978 - val_acc: 0.8675\n",
      "Epoch 5/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8574Epoch 00004: val_loss did not improve\n",
      "646866/646866 [==============================] - 439s - loss: 0.3157 - acc: 0.8574 - val_loss: 0.2978 - val_acc: 0.8684\n",
      "Epoch 6/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8584Epoch 00005: val_loss improved from 0.29777 to 0.29272, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.3122 - acc: 0.8584 - val_loss: 0.2927 - val_acc: 0.8692\n",
      "Epoch 7/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.8605Epoch 00006: val_loss did not improve\n",
      "646866/646866 [==============================] - 440s - loss: 0.3080 - acc: 0.8605 - val_loss: 0.2930 - val_acc: 0.8694\n",
      "Epoch 8/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.8615Epoch 00007: val_loss improved from 0.29272 to 0.28870, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 440s - loss: 0.3056 - acc: 0.8615 - val_loss: 0.2887 - val_acc: 0.8716\n",
      "Epoch 9/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.8629Epoch 00008: val_loss did not improve\n",
      "646866/646866 [==============================] - 440s - loss: 0.3032 - acc: 0.8629 - val_loss: 0.2913 - val_acc: 0.8684\n",
      "Epoch 10/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.8639Epoch 00009: val_loss improved from 0.28870 to 0.28729, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.3014 - acc: 0.8639 - val_loss: 0.2873 - val_acc: 0.8726\n",
      "Epoch 11/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.8651Epoch 00010: val_loss did not improve\n",
      "646866/646866 [==============================] - 439s - loss: 0.2996 - acc: 0.8651 - val_loss: 0.2891 - val_acc: 0.8704\n",
      "Epoch 12/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8655Epoch 00011: val_loss improved from 0.28729 to 0.28431, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.2980 - acc: 0.8655 - val_loss: 0.2843 - val_acc: 0.8728\n",
      "Epoch 13/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.8660Epoch 00012: val_loss improved from 0.28431 to 0.28202, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 439s - loss: 0.2963 - acc: 0.8660 - val_loss: 0.2820 - val_acc: 0.8751\n",
      "Epoch 14/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.8669Epoch 00013: val_loss did not improve\n",
      "646866/646866 [==============================] - 439s - loss: 0.2955 - acc: 0.8669 - val_loss: 0.2847 - val_acc: 0.8727\n",
      "Epoch 15/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.8674Epoch 00014: val_loss did not improve\n",
      "646866/646866 [==============================] - 439s - loss: 0.2937 - acc: 0.8674 - val_loss: 0.2844 - val_acc: 0.8739\n",
      "Epoch 16/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.8680Epoch 00015: val_loss did not improve\n",
      "646866/646866 [==============================] - 439s - loss: 0.2925 - acc: 0.8680 - val_loss: 0.2834 - val_acc: 0.8729\n",
      "Epoch 17/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.8676Epoch 00016: val_loss did not improve\n",
      "646866/646866 [==============================] - 439s - loss: 0.2946 - acc: 0.8676 - val_loss: 0.2833 - val_acc: 0.8743\n",
      "Epoch 00016: early stopping\n",
      "80857/80857 [==============================] - 3s     \n",
      "80857/80857 [==============================] - 3s     \n",
      "2345796/2345796 [==============================] - 107s   \n",
      "2344960/2345796 [============================>.] - ETA: 0s\n",
      "Fitting fold 5 of 5\n",
      "\n",
      "Train on 646866 samples, validate on 161714 samples\n",
      "Epoch 1/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8329Epoch 00000: val_loss improved from inf to 0.32515, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 445s - loss: 0.3770 - acc: 0.8329 - val_loss: 0.3252 - val_acc: 0.8590\n",
      "Epoch 2/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8450Epoch 00001: val_loss improved from 0.32515 to 0.30715, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 444s - loss: 0.3417 - acc: 0.8450 - val_loss: 0.3071 - val_acc: 0.8639\n",
      "Epoch 3/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8507Epoch 00002: val_loss improved from 0.30715 to 0.29788, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 445s - loss: 0.3286 - acc: 0.8507 - val_loss: 0.2979 - val_acc: 0.8695\n",
      "Epoch 4/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8553Epoch 00003: val_loss improved from 0.29788 to 0.29303, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 444s - loss: 0.3193 - acc: 0.8553 - val_loss: 0.2930 - val_acc: 0.8691\n",
      "Epoch 5/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8580Epoch 00004: val_loss improved from 0.29303 to 0.29117, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 444s - loss: 0.3128 - acc: 0.8580 - val_loss: 0.2912 - val_acc: 0.8729\n",
      "Epoch 6/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8601Epoch 00005: val_loss improved from 0.29117 to 0.28759, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 444s - loss: 0.3083 - acc: 0.8601 - val_loss: 0.2876 - val_acc: 0.8719\n",
      "Epoch 7/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.8666Epoch 00010: val_loss improved from 0.28240 to 0.28051, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 446s - loss: 0.2961 - acc: 0.8666 - val_loss: 0.2805 - val_acc: 0.8753\n",
      "Epoch 12/200\n",
      "646848/646866 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.8667Epoch 00011: val_loss improved from 0.28051 to 0.28022, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n",
      "646866/646866 [==============================] - 447s - loss: 0.2939 - acc: 0.8667 - val_loss: 0.2802 - val_acc: 0.8737\n",
      "Epoch 13/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.8677Epoch 00012: val_loss did not improve\n",
      "646866/646866 [==============================] - 447s - loss: 0.2926 - acc: 0.8677 - val_loss: 0.2837 - val_acc: 0.8727\n",
      "Epoch 14/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.8678Epoch 00013: val_loss improved from 0.28022 to 0.27968, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_magic.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646866/646866 [==============================] - 448s - loss: 0.2924 - acc: 0.8678 - val_loss: 0.2797 - val_acc: 0.8760\n",
      "Epoch 15/200\n",
      "646784/646866 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.8682Epoch 00014: val_loss did not improve\n",
      "646866/646866 [==============================] - 448s - loss: 0.2918 - acc: 0.8682 - val_loss: 0.2804 - val_acc: 0.8759\n",
      "Epoch 00014: early stopping\n",
      "80857/80857 [==============================] - 3s     \n",
      "80857/80857 [==============================] - 3s     \n",
      "2345796/2345796 [==============================] - 107s   \n",
      "2344960/2345796 [============================>.] - ETA: 0sCPU times: user 15h 3min 25s, sys: 1h 48s, total: 16h 4min 13s\n",
      "Wall time: 9h 24min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_q1, y_train)):\n",
    "    X_fold_train_q1 = np.vstack([X_train_q1[ix_train], X_train_q2[ix_train]])\n",
    "    X_fold_train_q2 = np.vstack([X_train_q2[ix_train], X_train_q1[ix_train]])\n",
    "    X_fold_train_magic = np.vstack([X_train_magic[ix_train], X_train_magic[ix_train]])\n",
    "\n",
    "    X_fold_val_q1 = np.vstack([X_train_q1[ix_val], X_train_q2[ix_val]])\n",
    "    X_fold_val_q2 = np.vstack([X_train_q2[ix_val], X_train_q1[ix_val]])\n",
    "    X_fold_val_magic = np.vstack([X_train_magic[ix_val], X_train_magic[ix_val]])\n",
    "\n",
    "    y_fold_train = np.concatenate([y_train[ix_train], y_train[ix_train]])\n",
    "    y_fold_val = np.concatenate([y_train[ix_val], y_train[ix_val]])\n",
    "    \n",
    "    print()\n",
    "    print(f'Fitting fold {fold_num + 1} of {kfold.n_splits}')\n",
    "    print()\n",
    "    \n",
    "    model = create_model(model_params)\n",
    "    model.fit(\n",
    "        [X_fold_train_q1, X_fold_train_q2, X_fold_train_magic], y_fold_train,\n",
    "        validation_data=([X_fold_val_q1, X_fold_val_q2, X_fold_val_magic], y_fold_val),\n",
    "\n",
    "        batch_size=64,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=1,\n",
    "        \n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.001,\n",
    "                patience=3,\n",
    "                verbose=1,\n",
    "                mode='auto',\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_checkpoint_path,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=2,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "        \n",
    "    # Create out-of-fold prediction.\n",
    "    model.load_weights(model_checkpoint_path)\n",
    "    \n",
    "    y_train_oofp[ix_val] = predict(model, X_train_q1[ix_val], X_train_q2[ix_val], X_train_magic[ix_val])\n",
    "    y_test_oofp[:, fold_num] = predict(model, X_test_q1, X_test_q2, X_test_magic)\n",
    "    \n",
    "    # Clear GPU memory.\n",
    "    K.clear_session()\n",
    "    del X_fold_train_q1\n",
    "    del X_fold_train_q2\n",
    "    del X_fold_train_magic\n",
    "    del X_fold_val_q1\n",
    "    del X_fold_val_q2\n",
    "    del X_fold_val_magic\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.28358329936\n"
     ]
    }
   ],
   "source": [
    "cv_score = log_loss(y_train, y_train_oofp)\n",
    "print('CV score:', cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'oofp_siamese_lstm_attention_magic',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_lines(feature_names, features_data_folder + f'X_train_{feature_list_id}.names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_oofp = y_train_oofp.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(y_train_oofp, features_data_folder + f'X_train_{feature_list_id}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_oofp_mean = np.mean(y_test_oofp, axis=1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(y_test_oofp_mean, features_data_folder + f'X_test_{feature_list_id}.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
