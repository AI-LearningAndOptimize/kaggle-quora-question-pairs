{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_use_gpus(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list_id = 'oofp_siamese_lstm_attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = load(aux_data_folder + 'embedding_weights_fasttext_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_q1 = load(features_data_folder + 'X_train_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_train_q2 = load(features_data_folder + 'X_train_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_q1 = load(features_data_folder + 'X_test_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_test_q2 = load(features_data_folder + 'X_test_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = load(features_data_folder + 'y_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[-1]\n",
    "VOCAB_LENGTH = embedding_matrix.shape[0]\n",
    "MAX_SEQUENCE_LENGTH = X_train_q1.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 101442 30\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDING_DIM, VOCAB_LENGTH, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models & Compute Out-of-Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean((1 - y_true) * K.square(y_pred) +\n",
    "                   y_true * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, init='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None,  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight((input_shape[-1], 1),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.b = self.add_weight((input_shape[1],),\n",
    "                                 initializer='zero',\n",
    "                                 name='{}_b'.format(self.name),\n",
    "                                 regularizer=self.bias_regularizer,\n",
    "                                 constraint=self.bias_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[1],),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, mask):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # (x, 40, 300) x (300, 1)\n",
    "        multData =  K.dot(x, self.kernel) # (x, 40, 1)\n",
    "        multData = K.squeeze(multData, -1) # (x, 40)\n",
    "        multData = multData + self.b # (x, 40) + (40,)\n",
    "\n",
    "        multData = K.tanh(multData) # (x, 40)\n",
    "\n",
    "        multData = multData * self.u # (x, 40) * (40, 1) => (x, 1)\n",
    "        multData = K.exp(multData) # (X, 1)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx()) #(x, 40)\n",
    "            multData = mask*multData #(x, 40) * (x, 40, )\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        multData /= K.cast(K.sum(multData, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        multData = K.expand_dims(multData)\n",
    "        weighted_input = x * multData\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    embedding_layer = Embedding(\n",
    "        VOCAB_LENGTH,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,\n",
    "    )\n",
    "    lstm_layer = LSTM(\n",
    "        params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "        return_sequences=True,\n",
    "    )\n",
    "    attention_layer = AttentionWithContext()\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = attention_layer(lstm_layer(embedded_sequences_1))\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = attention_layer(lstm_layer(embedded_sequences_2))\n",
    "\n",
    "    merged = concatenate([x1, y1])\n",
    "    merged = Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    merged = Dense(params['num_dense'], activation='relu')(merged)\n",
    "    merged = Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[sequence_1_input, sequence_2_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=contrastive_loss,\n",
    "        optimizer='nadam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint_path = aux_data_folder + 'fold-checkpoint-' + feature_list_id + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_oofp = np.zeros_like(y_train, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_oofp = np.zeros((len(X_test_q1), NUM_FOLDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X_q1, X_q2):\n",
    "    y1 = model.predict(\n",
    "        [X_q1, X_q2],\n",
    "        batch_size=1024,\n",
    "        verbose=1\n",
    "    ).reshape(-1)\n",
    "    \n",
    "    y2 = model.predict(\n",
    "        [X_q2, X_q1],\n",
    "        batch_size=1024,\n",
    "        verbose=1\n",
    "    ).reshape(-1)\n",
    "    \n",
    "    return (y1 + y2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'dense_dropout_rate': 0.164,\n",
    "    'lstm_dropout_rate': 0.324,\n",
    "    'num_dense': 132,\n",
    "    'num_lstm': 254,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting fold 1 of 5\n",
      "\n",
      "Train on 646862 samples, validate on 161718 samples\n",
      "Epoch 1/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.7147Epoch 00000: val_loss improved from inf to 0.20406, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 76s - loss: 0.1865 - acc: 0.7148 - val_loss: 0.2041 - val_acc: 0.6519\n",
      "Epoch 2/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.7659Epoch 00001: val_loss improved from 0.20406 to 0.15081, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 77s - loss: 0.1572 - acc: 0.7660 - val_loss: 0.1508 - val_acc: 0.7849\n",
      "Epoch 3/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.7889Epoch 00002: val_loss improved from 0.15081 to 0.13910, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 74s - loss: 0.1437 - acc: 0.7890 - val_loss: 0.1391 - val_acc: 0.7968\n",
      "Epoch 4/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.8039Epoch 00003: val_loss improved from 0.13910 to 0.12822, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 74s - loss: 0.1347 - acc: 0.8039 - val_loss: 0.1282 - val_acc: 0.8134\n",
      "Epoch 5/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.8162Epoch 00004: val_loss improved from 0.12822 to 0.12416, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 71s - loss: 0.1276 - acc: 0.8162 - val_loss: 0.1242 - val_acc: 0.8212\n",
      "Epoch 6/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.8258Epoch 00005: val_loss improved from 0.12416 to 0.11984, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 71s - loss: 0.1218 - acc: 0.8258 - val_loss: 0.1198 - val_acc: 0.8285\n",
      "Epoch 7/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.8327Epoch 00006: val_loss improved from 0.11984 to 0.11935, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 71s - loss: 0.1175 - acc: 0.8328 - val_loss: 0.1194 - val_acc: 0.8302\n",
      "Epoch 8/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.8395Epoch 00007: val_loss improved from 0.11935 to 0.11730, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 72s - loss: 0.1133 - acc: 0.8396 - val_loss: 0.1173 - val_acc: 0.8326\n",
      "Epoch 9/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.8450Epoch 00008: val_loss did not improve\n",
      "646862/646862 [==============================] - 75s - loss: 0.1100 - acc: 0.8450 - val_loss: 0.1183 - val_acc: 0.8316\n",
      "Epoch 10/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.8491Epoch 00009: val_loss did not improve\n",
      "646862/646862 [==============================] - 77s - loss: 0.1070 - acc: 0.8491 - val_loss: 0.1188 - val_acc: 0.8318\n",
      "Epoch 11/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.8534Epoch 00010: val_loss improved from 0.11730 to 0.11285, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 77s - loss: 0.1047 - acc: 0.8534 - val_loss: 0.1128 - val_acc: 0.8405\n",
      "Epoch 12/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.8577Epoch 00011: val_loss improved from 0.11285 to 0.11146, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 77s - loss: 0.1019 - acc: 0.8577 - val_loss: 0.1115 - val_acc: 0.8426\n",
      "Epoch 13/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.8604Epoch 00012: val_loss did not improve\n",
      "646862/646862 [==============================] - 77s - loss: 0.1001 - acc: 0.8604 - val_loss: 0.1143 - val_acc: 0.8396\n",
      "Epoch 14/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.8635Epoch 00013: val_loss did not improve\n",
      "646862/646862 [==============================] - 75s - loss: 0.0982 - acc: 0.8636 - val_loss: 0.1142 - val_acc: 0.8401\n",
      "Epoch 15/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.8662Epoch 00014: val_loss did not improve\n",
      "646862/646862 [==============================] - 75s - loss: 0.0966 - acc: 0.8662 - val_loss: 0.1140 - val_acc: 0.8398\n",
      "Epoch 16/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.8689Epoch 00015: val_loss improved from 0.11146 to 0.11067, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 76s - loss: 0.0948 - acc: 0.8690 - val_loss: 0.1107 - val_acc: 0.8448\n",
      "Epoch 00015: early stopping\n",
      "80859/80859 [==============================] - 4s     \n",
      "80859/80859 [==============================] - 4s     \n",
      "2344960/2345796 [============================>.] - ETA: 0s\n",
      "Fitting fold 2 of 5\n",
      "\n",
      "Train on 646862 samples, validate on 161718 samples\n",
      "Epoch 1/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.7159Epoch 00000: val_loss improved from inf to 0.22150, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 79s - loss: 0.1860 - acc: 0.7160 - val_loss: 0.2215 - val_acc: 0.6827\n",
      "Epoch 2/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.7676Epoch 00001: val_loss improved from 0.22150 to 0.15839, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 77s - loss: 0.1565 - acc: 0.7677 - val_loss: 0.1584 - val_acc: 0.7823\n",
      "Epoch 3/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.7892Epoch 00002: val_loss improved from 0.15839 to 0.13486, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 78s - loss: 0.1436 - acc: 0.7892 - val_loss: 0.1349 - val_acc: 0.8043\n",
      "Epoch 4/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8038Epoch 00003: val_loss improved from 0.13486 to 0.12904, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 78s - loss: 0.1349 - acc: 0.8037 - val_loss: 0.1290 - val_acc: 0.8144\n",
      "Epoch 5/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.8161Epoch 00004: val_loss improved from 0.12904 to 0.12806, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 76s - loss: 0.1280 - acc: 0.8161 - val_loss: 0.1281 - val_acc: 0.8134\n",
      "Epoch 6/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.8254Epoch 00005: val_loss improved from 0.12806 to 0.12084, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646862/646862 [==============================] - 77s - loss: 0.1220 - acc: 0.8253 - val_loss: 0.1208 - val_acc: 0.8269\n",
      "Epoch 7/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.8330Epoch 00006: val_loss improved from 0.12084 to 0.11983, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 75s - loss: 0.1175 - acc: 0.8330 - val_loss: 0.1198 - val_acc: 0.8275\n",
      "Epoch 8/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.8392Epoch 00007: val_loss did not improve\n",
      "646862/646862 [==============================] - 74s - loss: 0.1136 - acc: 0.8392 - val_loss: 0.1205 - val_acc: 0.8273\n",
      "Epoch 9/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.8450Epoch 00008: val_loss improved from 0.11983 to 0.11493, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 74s - loss: 0.1101 - acc: 0.8450 - val_loss: 0.1149 - val_acc: 0.8365\n",
      "Epoch 10/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.8496Epoch 00009: val_loss did not improve\n",
      "646862/646862 [==============================] - 78s - loss: 0.1073 - acc: 0.8495 - val_loss: 0.1179 - val_acc: 0.8314\n",
      "Epoch 11/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.8540Epoch 00010: val_loss improved from 0.11493 to 0.11465, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 77s - loss: 0.1044 - acc: 0.8540 - val_loss: 0.1147 - val_acc: 0.8365\n",
      "Epoch 12/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.8572Epoch 00011: val_loss did not improve\n",
      "646862/646862 [==============================] - 77s - loss: 0.1024 - acc: 0.8572 - val_loss: 0.1163 - val_acc: 0.8343\n",
      "Epoch 13/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.8603Epoch 00012: val_loss improved from 0.11465 to 0.11305, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 79s - loss: 0.1003 - acc: 0.8602 - val_loss: 0.1131 - val_acc: 0.8405\n",
      "Epoch 14/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.8636Epoch 00013: val_loss did not improve\n",
      "646862/646862 [==============================] - 78s - loss: 0.0983 - acc: 0.8636 - val_loss: 0.1161 - val_acc: 0.8368\n",
      "Epoch 15/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.8653Epoch 00014: val_loss did not improve\n",
      "646862/646862 [==============================] - 82s - loss: 0.0970 - acc: 0.8654 - val_loss: 0.1163 - val_acc: 0.8360\n",
      "Epoch 16/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.8675Epoch 00015: val_loss did not improve\n",
      "646862/646862 [==============================] - 82s - loss: 0.0955 - acc: 0.8675 - val_loss: 0.1161 - val_acc: 0.8369\n",
      "Epoch 17/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.8698Epoch 00016: val_loss improved from 0.11305 to 0.10740, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646862/646862 [==============================] - 78s - loss: 0.0942 - acc: 0.8698 - val_loss: 0.1074 - val_acc: 0.8503\n",
      "Epoch 18/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.8719Epoch 00017: val_loss did not improve\n",
      "646862/646862 [==============================] - 73s - loss: 0.0928 - acc: 0.8719 - val_loss: 0.1105 - val_acc: 0.8451\n",
      "Epoch 19/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.8734Epoch 00018: val_loss did not improve\n",
      "646862/646862 [==============================] - 72s - loss: 0.0918 - acc: 0.8734 - val_loss: 0.1123 - val_acc: 0.8421\n",
      "Epoch 20/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.8752Epoch 00019: val_loss did not improve\n",
      "646862/646862 [==============================] - 74s - loss: 0.0907 - acc: 0.8752 - val_loss: 0.1206 - val_acc: 0.8307\n",
      "Epoch 21/200\n",
      "645120/646862 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.8762Epoch 00020: val_loss did not improve\n",
      "646862/646862 [==============================] - 71s - loss: 0.0898 - acc: 0.8762 - val_loss: 0.1121 - val_acc: 0.8434\n",
      "Epoch 00020: early stopping\n",
      "80859/80859 [==============================] - 3s     \n",
      "80859/80859 [==============================] - 3s     \n",
      "2345796/2345796 [==============================] - 108s   \n",
      "\n",
      "Fitting fold 3 of 5\n",
      "\n",
      "Train on 646864 samples, validate on 161716 samples\n",
      "Epoch 1/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.7173Epoch 00000: val_loss improved from inf to 0.21261, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 76s - loss: 0.1846 - acc: 0.7174 - val_loss: 0.2126 - val_acc: 0.6799\n",
      "Epoch 2/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.7698Epoch 00001: val_loss improved from 0.21261 to 0.15305, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 76s - loss: 0.1549 - acc: 0.7698 - val_loss: 0.1531 - val_acc: 0.7829\n",
      "Epoch 3/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.7924Epoch 00002: val_loss improved from 0.15305 to 0.13609, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 76s - loss: 0.1417 - acc: 0.7924 - val_loss: 0.1361 - val_acc: 0.8020\n",
      "Epoch 4/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.8067Epoch 00003: val_loss improved from 0.13609 to 0.12689, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 75s - loss: 0.1332 - acc: 0.8068 - val_loss: 0.1269 - val_acc: 0.8169\n",
      "Epoch 5/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.8183Epoch 00004: val_loss improved from 0.12689 to 0.12585, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 77s - loss: 0.1266 - acc: 0.8183 - val_loss: 0.1259 - val_acc: 0.8192\n",
      "Epoch 6/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.8264Epoch 00005: val_loss improved from 0.12585 to 0.12179, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 72s - loss: 0.1212 - acc: 0.8264 - val_loss: 0.1218 - val_acc: 0.8265\n",
      "Epoch 7/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.8342Epoch 00006: val_loss improved from 0.12179 to 0.11904, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 75s - loss: 0.1165 - acc: 0.8342 - val_loss: 0.1190 - val_acc: 0.8304\n",
      "Epoch 8/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.8410Epoch 00007: val_loss did not improve\n",
      "646864/646864 [==============================] - 76s - loss: 0.1125 - acc: 0.8410 - val_loss: 0.1203 - val_acc: 0.8282\n",
      "Epoch 9/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.8455Epoch 00008: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646864/646864 [==============================] - 75s - loss: 0.1095 - acc: 0.8455 - val_loss: 0.1209 - val_acc: 0.8272\n",
      "Epoch 10/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.8504Epoch 00009: val_loss improved from 0.11904 to 0.11724, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 76s - loss: 0.1066 - acc: 0.8504 - val_loss: 0.1172 - val_acc: 0.8321\n",
      "Epoch 11/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.8542Epoch 00010: val_loss improved from 0.11724 to 0.11412, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 78s - loss: 0.1041 - acc: 0.8541 - val_loss: 0.1141 - val_acc: 0.8384\n",
      "Epoch 12/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.8582Epoch 00011: val_loss did not improve\n",
      "646864/646864 [==============================] - 79s - loss: 0.1017 - acc: 0.8582 - val_loss: 0.1169 - val_acc: 0.8346\n",
      "Epoch 13/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.8611Epoch 00012: val_loss improved from 0.11412 to 0.11246, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646864/646864 [==============================] - 77s - loss: 0.0998 - acc: 0.8611 - val_loss: 0.1125 - val_acc: 0.8413\n",
      "Epoch 14/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.8642Epoch 00013: val_loss did not improve\n",
      "646864/646864 [==============================] - 77s - loss: 0.0978 - acc: 0.8642 - val_loss: 0.1169 - val_acc: 0.8352\n",
      "Epoch 15/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.8667Epoch 00014: val_loss did not improve\n",
      "646864/646864 [==============================] - 77s - loss: 0.0961 - acc: 0.8666 - val_loss: 0.1137 - val_acc: 0.8409\n",
      "Epoch 16/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.8682Epoch 00015: val_loss did not improve\n",
      "646864/646864 [==============================] - 79s - loss: 0.0950 - acc: 0.8682 - val_loss: 0.1150 - val_acc: 0.8377\n",
      "Epoch 17/200\n",
      "645120/646864 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.8709Epoch 00016: val_loss did not improve\n",
      "646864/646864 [==============================] - 79s - loss: 0.0935 - acc: 0.8709 - val_loss: 0.1132 - val_acc: 0.8410\n",
      "Epoch 00016: early stopping\n",
      "80858/80858 [==============================] - 3s     \n",
      "2344960/2345796 [============================>.] - ETA: 0s\n",
      "Fitting fold 4 of 5\n",
      "\n",
      "Train on 646866 samples, validate on 161714 samples\n",
      "Epoch 1/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.7206Epoch 00000: val_loss improved from inf to 0.22812, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 78s - loss: 0.1831 - acc: 0.7206 - val_loss: 0.2281 - val_acc: 0.6115\n",
      "Epoch 2/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.7699Epoch 00001: val_loss improved from 0.22812 to 0.15661, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 75s - loss: 0.1552 - acc: 0.7699 - val_loss: 0.1566 - val_acc: 0.7708\n",
      "Epoch 3/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.7915Epoch 00002: val_loss improved from 0.15661 to 0.13333, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 79s - loss: 0.1421 - acc: 0.7915 - val_loss: 0.1333 - val_acc: 0.8059\n",
      "Epoch 4/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.8065Epoch 00003: val_loss improved from 0.13333 to 0.12754, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 80s - loss: 0.1336 - acc: 0.8065 - val_loss: 0.1275 - val_acc: 0.8150\n",
      "Epoch 5/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.8174Epoch 00004: val_loss improved from 0.12754 to 0.12533, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 79s - loss: 0.1270 - acc: 0.8174 - val_loss: 0.1253 - val_acc: 0.8195\n",
      "Epoch 6/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.8267Epoch 00005: val_loss improved from 0.12533 to 0.12078, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 77s - loss: 0.1214 - acc: 0.8266 - val_loss: 0.1208 - val_acc: 0.8268\n",
      "Epoch 7/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.8332Epoch 00006: val_loss improved from 0.12078 to 0.11952, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 76s - loss: 0.1171 - acc: 0.8332 - val_loss: 0.1195 - val_acc: 0.8281\n",
      "Epoch 8/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.8392Epoch 00007: val_loss did not improve\n",
      "646866/646866 [==============================] - 76s - loss: 0.1134 - acc: 0.8392 - val_loss: 0.1212 - val_acc: 0.8266\n",
      "Epoch 9/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.8453Epoch 00008: val_loss improved from 0.11952 to 0.11836, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 77s - loss: 0.1098 - acc: 0.8452 - val_loss: 0.1184 - val_acc: 0.8310\n",
      "Epoch 10/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.8488Epoch 00009: val_loss improved from 0.11836 to 0.11810, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 76s - loss: 0.1073 - acc: 0.8488 - val_loss: 0.1181 - val_acc: 0.8318\n",
      "Epoch 11/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.8536Epoch 00010: val_loss did not improve\n",
      "646866/646866 [==============================] - 76s - loss: 0.1045 - acc: 0.8536 - val_loss: 0.1182 - val_acc: 0.8310\n",
      "Epoch 12/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.8568Epoch 00011: val_loss improved from 0.11810 to 0.11453, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 77s - loss: 0.1024 - acc: 0.8569 - val_loss: 0.1145 - val_acc: 0.8384\n",
      "Epoch 13/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.8604Epoch 00012: val_loss did not improve\n",
      "646866/646866 [==============================] - 79s - loss: 0.1002 - acc: 0.8604 - val_loss: 0.1165 - val_acc: 0.8346\n",
      "Epoch 14/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.8632Epoch 00013: val_loss improved from 0.11453 to 0.11394, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 78s - loss: 0.0984 - acc: 0.8632 - val_loss: 0.1139 - val_acc: 0.8380\n",
      "Epoch 15/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.8655Epoch 00014: val_loss improved from 0.11394 to 0.11201, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646866/646866 [==============================] - 78s - loss: 0.0968 - acc: 0.8655 - val_loss: 0.1120 - val_acc: 0.8422\n",
      "Epoch 16/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.8673Epoch 00015: val_loss did not improve\n",
      "646866/646866 [==============================] - 78s - loss: 0.0956 - acc: 0.8673 - val_loss: 0.1130 - val_acc: 0.8427\n",
      "Epoch 17/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.8697Epoch 00016: val_loss did not improve\n",
      "646866/646866 [==============================] - 79s - loss: 0.0941 - acc: 0.8697 - val_loss: 0.1139 - val_acc: 0.8400\n",
      "Epoch 18/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.8712Epoch 00017: val_loss improved from 0.11201 to 0.11180, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 81s - loss: 0.0929 - acc: 0.8712 - val_loss: 0.1118 - val_acc: 0.8440\n",
      "Epoch 19/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.8733Epoch 00018: val_loss did not improve\n",
      "646866/646866 [==============================] - 81s - loss: 0.0920 - acc: 0.8733 - val_loss: 0.1127 - val_acc: 0.8429\n",
      "Epoch 00018: early stopping\n",
      "80857/80857 [==============================] - 4s     \n",
      "80857/80857 [==============================] - 4s     \n",
      "2345796/2345796 [==============================] - 121s   \n",
      "\n",
      "Fitting fold 5 of 5\n",
      "\n",
      "Train on 646866 samples, validate on 161714 samples\n",
      "Epoch 1/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.7166Epoch 00000: val_loss improved from inf to 0.19973, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 83s - loss: 0.1847 - acc: 0.7167 - val_loss: 0.1997 - val_acc: 0.6955\n",
      "Epoch 2/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.7688Epoch 00001: val_loss improved from 0.19973 to 0.15274, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 80s - loss: 0.1553 - acc: 0.7688 - val_loss: 0.1527 - val_acc: 0.7875\n",
      "Epoch 3/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.7912Epoch 00002: val_loss improved from 0.15274 to 0.13040, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 77s - loss: 0.1422 - acc: 0.7912 - val_loss: 0.1304 - val_acc: 0.8121\n",
      "Epoch 4/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.8058Epoch 00003: val_loss improved from 0.13040 to 0.13022, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 79s - loss: 0.1336 - acc: 0.8058 - val_loss: 0.1302 - val_acc: 0.8101\n",
      "Epoch 5/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.8178Epoch 00004: val_loss improved from 0.13022 to 0.12195, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 79s - loss: 0.1267 - acc: 0.8178 - val_loss: 0.1219 - val_acc: 0.8246\n",
      "Epoch 6/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.8262Epoch 00005: val_loss did not improve\n",
      "646866/646866 [==============================] - 80s - loss: 0.1215 - acc: 0.8262 - val_loss: 0.1235 - val_acc: 0.8237\n",
      "Epoch 7/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.8335Epoch 00006: val_loss improved from 0.12195 to 0.11665, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 81s - loss: 0.1171 - acc: 0.8334 - val_loss: 0.1167 - val_acc: 0.8329\n",
      "Epoch 8/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.8396Epoch 00007: val_loss did not improve\n",
      "646866/646866 [==============================] - 79s - loss: 0.1134 - acc: 0.8395 - val_loss: 0.1199 - val_acc: 0.8291\n",
      "Epoch 9/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.8453Epoch 00008: val_loss did not improve\n",
      "646866/646866 [==============================] - 80s - loss: 0.1097 - acc: 0.8453 - val_loss: 0.1172 - val_acc: 0.8324\n",
      "Epoch 10/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.8491Epoch 00009: val_loss improved from 0.11665 to 0.11525, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 78s - loss: 0.1073 - acc: 0.8491 - val_loss: 0.1152 - val_acc: 0.8367\n",
      "Epoch 11/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.8538Epoch 00010: val_loss did not improve\n",
      "646866/646866 [==============================] - 77s - loss: 0.1046 - acc: 0.8538 - val_loss: 0.1154 - val_acc: 0.8354\n",
      "Epoch 12/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.8572Epoch 00011: val_loss improved from 0.11525 to 0.11208, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 76s - loss: 0.1022 - acc: 0.8572 - val_loss: 0.1121 - val_acc: 0.8413\n",
      "Epoch 13/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.8601Epoch 00012: val_loss did not improve\n",
      "646866/646866 [==============================] - 78s - loss: 0.1001 - acc: 0.8601 - val_loss: 0.1134 - val_acc: 0.8389\n",
      "Epoch 14/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.8631Epoch 00013: val_loss improved from 0.11208 to 0.11022, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention.h5\n",
      "646866/646866 [==============================] - 77s - loss: 0.0985 - acc: 0.8631 - val_loss: 0.1102 - val_acc: 0.8451\n",
      "Epoch 15/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.8648Epoch 00014: val_loss did not improve\n",
      "646866/646866 [==============================] - 73s - loss: 0.0973 - acc: 0.8648 - val_loss: 0.1106 - val_acc: 0.8447\n",
      "Epoch 16/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.8675Epoch 00015: val_loss did not improve\n",
      "646866/646866 [==============================] - 78s - loss: 0.0955 - acc: 0.8675 - val_loss: 0.1116 - val_acc: 0.8434\n",
      "Epoch 17/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.8690Epoch 00016: val_loss did not improve\n",
      "646866/646866 [==============================] - 77s - loss: 0.0944 - acc: 0.8690 - val_loss: 0.1129 - val_acc: 0.8414\n",
      "Epoch 18/200\n",
      "645120/646866 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.8713Epoch 00017: val_loss did not improve\n",
      "646866/646866 [==============================] - 77s - loss: 0.0929 - acc: 0.8713 - val_loss: 0.1112 - val_acc: 0.8450\n",
      "Epoch 00017: early stopping\n",
      "80857/80857 [==============================] - 3s     \n",
      "2344960/2345796 [============================>.] - ETA: 0sCPU times: user 1h 38min 20s, sys: 18min 9s, total: 1h 56min 30s\n",
      "Wall time: 2h 17min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_q1, y_train)):\n",
    "    X_fold_train_q1 = np.vstack([X_train_q1[ix_train], X_train_q2[ix_train]])\n",
    "    X_fold_train_q2 = np.vstack([X_train_q2[ix_train], X_train_q1[ix_train]])\n",
    "\n",
    "    X_fold_val_q1 = np.vstack([X_train_q1[ix_val], X_train_q2[ix_val]])\n",
    "    X_fold_val_q2 = np.vstack([X_train_q2[ix_val], X_train_q1[ix_val]])\n",
    "\n",
    "    y_fold_train = np.concatenate([y_train[ix_train], y_train[ix_train]])\n",
    "    y_fold_val = np.concatenate([y_train[ix_val], y_train[ix_val]])\n",
    "    \n",
    "    print()\n",
    "    print(f'Fitting fold {fold_num + 1} of {kfold.n_splits}')\n",
    "    print()\n",
    "    \n",
    "    model = create_model(model_params)\n",
    "    model.fit(\n",
    "        [X_fold_train_q1, X_fold_train_q2], y_fold_train,\n",
    "        validation_data=([X_fold_val_q1, X_fold_val_q2], y_fold_val),\n",
    "\n",
    "        batch_size=2048,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=1,\n",
    "        \n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.001,\n",
    "                patience=3,\n",
    "                verbose=1,\n",
    "                mode='auto',\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_checkpoint_path,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=2,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "        \n",
    "    # Create out-of-fold prediction.\n",
    "    model.load_weights(model_checkpoint_path)\n",
    "    \n",
    "    y_train_oofp[ix_val] = predict(model, X_train_q1[ix_val], X_train_q2[ix_val])\n",
    "    y_test_oofp[:, fold_num] = predict(model, X_test_q1, X_test_q2)\n",
    "    \n",
    "    # Clear GPU memory.\n",
    "    K.clear_session()\n",
    "    del X_fold_train_q1\n",
    "    del X_fold_train_q2\n",
    "    del X_fold_val_q1\n",
    "    del X_fold_val_q2\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.357949355293\n"
     ]
    }
   ],
   "source": [
    "cv_score = log_loss(y_train, y_train_oofp)\n",
    "print('CV score:', cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'oofp_siamese_lstm_attention',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_lines(feature_names, features_data_folder + f'X_train_{feature_list_id}.names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_oofp = y_train_oofp.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(y_train_oofp, features_data_folder + f'X_train_{feature_list_id}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_oofp_mean = np.mean(y_test_oofp, axis=1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(y_test_oofp_mean, features_data_folder + f'X_test_{feature_list_id}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2b9430c898>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAGoCAYAAACZh1c1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf76BvGdQ0BJAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b942eaba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y_test_oofp_mean).plot.hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
