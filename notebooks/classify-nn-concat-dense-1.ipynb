{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_use_gpus(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = os.path.abspath(os.path.join(os.curdir, os.pardir, 'data')) + os.path.sep\n",
    "aux_data_folder = os.path.join(data_folder, 'aux') + os.path.sep\n",
    "preproc_data_folder = os.path.join(data_folder, 'preproc') + os.path.sep\n",
    "features_data_folder = os.path.join(data_folder, 'features') + os.path.sep\n",
    "submissions_data_folder = os.path.join(data_folder, 'submissions') + os.path.sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = load(aux_data_folder + 'embedding_weights_fasttext_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_q1 = load(features_data_folder + 'X_train_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_train_q2 = load(features_data_folder + 'X_train_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.stack((X_train_q1, X_train_q2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = load(features_data_folder + 'y_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[-1]\n",
    "VOCAB_LENGTH = embedding_matrix.shape[0] - 1\n",
    "MAX_SEQUENCE_LENGTH = X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 101441 30\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDING_DIM, VOCAB_LENGTH, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (343646, 2, 30)\n",
      "y train: (343646,)\n",
      "X val:   (60644, 2, 30)\n",
      "y val:   (60644,)\n"
     ]
    }
   ],
   "source": [
    "print('X train:', X_train.shape)\n",
    "print('y train:', y_train.shape)\n",
    "print('X val:  ', X_val.shape)\n",
    "print('y val:  ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_q1 = X_train[:, 0]\n",
    "X_train_q2 = X_train[:, 1]\n",
    "X_val_q1 = X_val[:, 0]\n",
    "X_val_q2 = X_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train Q1: (343646, 30)\n",
      "y train Q1: (343646, 30)\n",
      "X val Q1:   (60644, 30)\n",
      "y val Q1:   (60644, 30)\n"
     ]
    }
   ],
   "source": [
    "print('X train Q1:', X_train_q1.shape)\n",
    "print('y train Q1:', X_train_q2.shape)\n",
    "print('X val Q1:  ', X_val_q1.shape)\n",
    "print('y val Q1:  ', X_val_q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_q1 = Sequential()\n",
    "\n",
    "model_q1.add(Embedding(\n",
    "    VOCAB_LENGTH + 1,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False,\n",
    "))\n",
    "\n",
    "model_q1.add(TimeDistributed(Dense(\n",
    "    EMBEDDING_DIM,\n",
    "    activation='relu',\n",
    ")))\n",
    "\n",
    "model_q1.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_q2 = Sequential()\n",
    "\n",
    "model_q2.add(Embedding(\n",
    "    VOCAB_LENGTH + 1,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False,\n",
    "))\n",
    "\n",
    "model_q2.add(TimeDistributed(Dense(\n",
    "    EMBEDDING_DIM,\n",
    "    activation='relu'\n",
    ")))\n",
    "\n",
    "model_q2.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuriyguts/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Merge([model_q1, model_q2], mode='concat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               120200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 61,292,401.0\n",
      "Trainable params: 424,401.0\n",
      "Non-trainable params: 60,868,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_weights(y, smooth_factor=0):\n",
    "    \"\"\"\n",
    "    Returns the weights for each class based on the frequencies of the samples\n",
    "    :param smooth_factor: factor that smooths extremely uneven weights\n",
    "    :param y: list of true labels (the labels must be hashable)\n",
    "    :return: dictionary with the weight for each class\n",
    "    \"\"\"\n",
    "    from collections import Counter    \n",
    "    counter = Counter(y)\n",
    "\n",
    "    if smooth_factor > 0:\n",
    "        p = max(counter.values()) * smooth_factor\n",
    "        for k in counter.keys():\n",
    "            counter[k] += p\n",
    "\n",
    "    majority = max(counter.values())\n",
    "\n",
    "    return {cls: float(majority / count) for cls, count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343646 samples, validate on 60644 samples\n",
      "Epoch 1/15\n",
      "343646/343646 [==============================] - 31s - loss: 0.6522 - acc: 0.7347 - val_loss: 0.4578 - val_acc: 0.7730\n",
      "Epoch 2/15\n",
      "343646/343646 [==============================] - 31s - loss: 0.5562 - acc: 0.7822 - val_loss: 0.4263 - val_acc: 0.7975\n",
      "Epoch 3/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.5019 - acc: 0.8073 - val_loss: 0.4233 - val_acc: 0.7967\n",
      "Epoch 4/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.4590 - acc: 0.8267 - val_loss: 0.4060 - val_acc: 0.8055\n",
      "Epoch 5/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.4222 - acc: 0.8429 - val_loss: 0.3977 - val_acc: 0.8114\n",
      "Epoch 6/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.3931 - acc: 0.8550 - val_loss: 0.3913 - val_acc: 0.8191\n",
      "Epoch 7/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.3642 - acc: 0.8671 - val_loss: 0.4047 - val_acc: 0.8178\n",
      "Epoch 8/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.3409 - acc: 0.8770 - val_loss: 0.4045 - val_acc: 0.8225\n",
      "Epoch 9/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.3191 - acc: 0.8863 - val_loss: 0.4227 - val_acc: 0.8215\n",
      "Epoch 10/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.2999 - acc: 0.8937 - val_loss: 0.4299 - val_acc: 0.8244\n",
      "Epoch 11/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.2820 - acc: 0.9005 - val_loss: 0.4433 - val_acc: 0.8229\n",
      "Epoch 12/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.2672 - acc: 0.9073 - val_loss: 0.4275 - val_acc: 0.8283\n",
      "Epoch 13/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.2540 - acc: 0.9121 - val_loss: 0.4701 - val_acc: 0.8119\n",
      "Epoch 14/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.2425 - acc: 0.9167 - val_loss: 0.4617 - val_acc: 0.8280\n",
      "Epoch 15/15\n",
      "343646/343646 [==============================] - 32s - loss: 0.2307 - acc: 0.9214 - val_loss: 0.4643 - val_acc: 0.8259\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_q1, X_train_q2], y_train,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    validation_data=([X_val_q1, X_val_q2], y_val),\n",
    "    class_weight=get_class_weights(y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    " \n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "plt.title(\"Loss over time\")\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict_classes([X_train_q1, X_train_q2])[:, -1]\n",
    "y_pred_proba_train = model.predict_proba([X_train_q1, X_train_q2])[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model.predict_classes([X_val_q1, X_val_q2])[:, -1]\n",
    "y_pred_proba_val = model.predict_proba([X_val_q1, X_val_q2])[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_metrics = [log_loss, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_metrics = [accuracy_score, precision_score, recall_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in continuous_metrics:\n",
    "    print('{:20s}: {:10.5f}'.format(metric.__name__, metric(y_train, y_pred_proba_train)))\n",
    "for metric in binary_metrics:\n",
    "    print('{:20s}: {:10.5f}'.format(metric.__name__, metric(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in continuous_metrics:\n",
    "    print('{:20s}: {:10.5f}'.format(metric.__name__, metric(y_val, y_pred_proba_val)))\n",
    "for metric in binary_metrics:\n",
    "    print('{:20s}: {:10.5f}'.format(metric.__name__, metric(y_val, y_pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('Stopping before the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_q1 = load(features_data_folder + 'X_test_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_test_q2 = load(features_data_folder + 'X_test_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = model.predict_proba([X_test_q1, X_test_q2])[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_id = datetime.datetime.now().strftime('%Y-%m-%d-%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({\n",
    "    'test_id': range(len(y_test)),\n",
    "    'is_duplicate': y_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission = df_submission[['test_id', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.to_csv(\n",
    "    submissions_data_folder + submission_id + '-submission-draft.csv',\n",
    "    header=True,\n",
    "    float_format='%.8f',\n",
    "    index=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
