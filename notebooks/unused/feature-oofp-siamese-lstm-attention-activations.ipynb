{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_use_gpus(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list_id = 'oofp_siamese_lstm_attention_activations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = load(aux_data_folder + 'embedding_weights_fasttext_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_q1 = load(features_data_folder + 'X_train_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_train_q2 = load(features_data_folder + 'X_train_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_q1 = load(features_data_folder + 'X_test_nn_fasttext_q1_filtered_no_stopwords.pickle')\n",
    "X_test_q2 = load(features_data_folder + 'X_test_nn_fasttext_q2_filtered_no_stopwords.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = load(features_data_folder + 'y_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[-1]\n",
    "VOCAB_LENGTH = embedding_matrix.shape[0]\n",
    "MAX_SEQUENCE_LENGTH = X_train_q1.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 101442 30\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDING_DIM, VOCAB_LENGTH, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models & Compute Out-of-Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'dense_dropout_rate': 0.164,\n",
    "    'lstm_dropout_rate': 0.324,\n",
    "    'num_dense_1': 128,\n",
    "    'num_dense_2': 64,\n",
    "    'num_lstm': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, init='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None,  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight((input_shape[-1], 1),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.b = self.add_weight((input_shape[1],),\n",
    "                                 initializer='zero',\n",
    "                                 name='{}_b'.format(self.name),\n",
    "                                 regularizer=self.bias_regularizer,\n",
    "                                 constraint=self.bias_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[1],),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, mask):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # (x, 40, 300) x (300, 1)\n",
    "        multData =  K.dot(x, self.kernel) # (x, 40, 1)\n",
    "        multData = K.squeeze(multData, -1) # (x, 40)\n",
    "        multData = multData + self.b # (x, 40) + (40,)\n",
    "\n",
    "        multData = K.tanh(multData) # (x, 40)\n",
    "\n",
    "        multData = multData * self.u # (x, 40) * (40, 1) => (x, 1)\n",
    "        multData = K.exp(multData) # (X, 1)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx()) #(x, 40)\n",
    "            multData = mask*multData #(x, 40) * (x, 40, )\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        multData /= K.cast(K.sum(multData, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        multData = K.expand_dims(multData)\n",
    "        weighted_input = x * multData\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dense_block(input_layer, num_units, dropout_rate):\n",
    "    dense = Dense(num_units)(input_layer)\n",
    "    bn = BatchNormalization()(dense)\n",
    "    relu = Activation('relu')(bn)\n",
    "    dropout = Dropout(dropout_rate)(relu)\n",
    "    output = dropout\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    embedding_layer = Embedding(\n",
    "        VOCAB_LENGTH,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,\n",
    "    )\n",
    "    lstm_layer = LSTM(\n",
    "        params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "        return_sequences=True,\n",
    "    )\n",
    "    attention_layer = AttentionWithContext()\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = attention_layer(lstm_layer(embedded_sequences_1))\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = attention_layer(lstm_layer(embedded_sequences_2))\n",
    "\n",
    "    merged = Concatenate(name='feature_output')([x1, y1])\n",
    "    dropout = Dropout(params['dense_dropout_rate'])(merged)\n",
    "    \n",
    "    dense_1 = create_dense_block(dropout, params['num_dense_1'], params['dense_dropout_rate'])\n",
    "    dense_2 = create_dense_block(dropout, params['num_dense_2'], params['dense_dropout_rate'])\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', name='target_output')(dense_2)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[sequence_1_input, sequence_2_input],\n",
    "        outputs=[output, merged],\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss={'target_output': 'binary_crossentropy', 'feature_output': zero_loss},\n",
    "        loss_weights={'target_output': 1.0, 'feature_output': 0.0},\n",
    "        optimizer='nadam',\n",
    "        metrics=None\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_output_size = model_params['num_lstm'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint_path = aux_data_folder + 'fold-checkpoint-' + feature_list_id + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_oofp = np.zeros_like(y_train, dtype='float32')\n",
    "y_train_oofp_features = np.zeros((len(y_train), feature_output_size), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_oofp = np.zeros((len(X_test_q1), NUM_FOLDS), dtype='float32')\n",
    "y_test_oofp_features = np.zeros((len(X_test_q1), feature_output_size), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting fold 1 of 5\n",
      "\n",
      "Train on 323431 samples, validate on 80859 samples\n",
      "Epoch 1/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.5458 - target_output_loss: 0.5458 - feature_output_loss: 0.0000e+00Epoch 00000: val_loss improved from inf to 0.48930, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 55s - loss: 0.5458 - target_output_loss: 0.5458 - feature_output_loss: 0.0000e+00 - val_loss: 0.4893 - val_target_output_loss: 0.4893 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 2/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4825 - target_output_loss: 0.4825 - feature_output_loss: 0.0000e+00 ETA: 2s - loss: 0.4827 - target_output_lossEpoch 00001: val_loss improved from 0.48930 to 0.45187, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 63s - loss: 0.4825 - target_output_loss: 0.4825 - feature_output_loss: 0.0000e+00 - val_loss: 0.4519 - val_target_output_loss: 0.4519 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 3/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4507 - target_output_loss: 0.4507 - feature_output_loss: 0.0000e+00Epoch 00002: val_loss improved from 0.45187 to 0.41712, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 60s - loss: 0.4507 - target_output_loss: 0.4507 - feature_output_loss: 0.0000e+00 - val_loss: 0.4171 - val_target_output_loss: 0.4171 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 4/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4300 - target_output_loss: 0.4300 - feature_output_loss: 0.0000e+00Epoch 00003: val_loss improved from 0.41712 to 0.40543, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 61s - loss: 0.4300 - target_output_loss: 0.4300 - feature_output_loss: 0.0000e+00 - val_loss: 0.4054 - val_target_output_loss: 0.4054 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 5/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4138 - target_output_loss: 0.4138 - feature_output_loss: 0.0000e+00Epoch 00004: val_loss improved from 0.40543 to 0.40411, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 59s - loss: 0.4138 - target_output_loss: 0.4138 - feature_output_loss: 0.0000e+00 - val_loss: 0.4041 - val_target_output_loss: 0.4041 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 6/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4009 - target_output_loss: 0.4009 - feature_output_loss: 0.0000e+00Epoch 00005: val_loss improved from 0.40411 to 0.38444, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.4009 - target_output_loss: 0.4009 - feature_output_loss: 0.0000e+00 - val_loss: 0.3844 - val_target_output_loss: 0.3844 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 7/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3898 - target_output_loss: 0.3898 - feature_output_loss: 0.0000e+00Epoch 00006: val_loss improved from 0.38444 to 0.38352, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.3898 - target_output_loss: 0.3898 - feature_output_loss: 0.0000e+00 - val_loss: 0.3835 - val_target_output_loss: 0.3835 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 8/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3811 - target_output_loss: 0.3811 - feature_output_loss: 0.0000e+00Epoch 00007: val_loss improved from 0.38352 to 0.37700, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 60s - loss: 0.3812 - target_output_loss: 0.3812 - feature_output_loss: 0.0000e+00 - val_loss: 0.3770 - val_target_output_loss: 0.3770 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 9/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3711 - target_output_loss: 0.3711 - feature_output_loss: 0.0000e+00Epoch 00008: val_loss improved from 0.37700 to 0.37036, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.3711 - target_output_loss: 0.3711 - feature_output_loss: 0.0000e+00 - val_loss: 0.3704 - val_target_output_loss: 0.3704 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 10/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3642 - target_output_loss: 0.3642 - feature_output_loss: 0.0000e+00Epoch 00009: val_loss improved from 0.37036 to 0.36939, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 56s - loss: 0.3643 - target_output_loss: 0.3643 - feature_output_loss: 0.0000e+00 - val_loss: 0.3694 - val_target_output_loss: 0.3694 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 11/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3578 - target_output_loss: 0.3578 - feature_output_loss: 0.0000e+00Epoch 00010: val_loss improved from 0.36939 to 0.36414, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 55s - loss: 0.3579 - target_output_loss: 0.3579 - feature_output_loss: 0.0000e+00 - val_loss: 0.3641 - val_target_output_loss: 0.3641 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 12/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3542 - target_output_loss: 0.3542 - feature_output_loss: 0.0000e+00Epoch 00011: val_loss did not improve\n",
      "323431/323431 [==============================] - 54s - loss: 0.3542 - target_output_loss: 0.3542 - feature_output_loss: 0.0000e+00 - val_loss: 0.3672 - val_target_output_loss: 0.3672 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 13/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3479 - target_output_loss: 0.3479 - feature_output_loss: 0.0000e+00Epoch 00012: val_loss improved from 0.36414 to 0.35701, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 53s - loss: 0.3479 - target_output_loss: 0.3479 - feature_output_loss: 0.0000e+00 - val_loss: 0.3570 - val_target_output_loss: 0.3570 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 14/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3433 - target_output_loss: 0.3433 - feature_output_loss: 0.0000e+00Epoch 00013: val_loss improved from 0.35701 to 0.35651, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 54s - loss: 0.3433 - target_output_loss: 0.3433 - feature_output_loss: 0.0000e+00 - val_loss: 0.3565 - val_target_output_loss: 0.3565 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 15/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3389 - target_output_loss: 0.3389 - feature_output_loss: 0.0000e+00Epoch 00014: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323431/323431 [==============================] - 57s - loss: 0.3389 - target_output_loss: 0.3389 - feature_output_loss: 0.0000e+00 - val_loss: 0.3595 - val_target_output_loss: 0.3595 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 16/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3355 - target_output_loss: 0.3355 - feature_output_loss: 0.0000e+00Epoch 00015: val_loss improved from 0.35651 to 0.35171, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 57s - loss: 0.3355 - target_output_loss: 0.3355 - feature_output_loss: 0.0000e+00 - val_loss: 0.3517 - val_target_output_loss: 0.3517 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 17/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3323 - target_output_loss: 0.3323 - feature_output_loss: 0.0000e+00Epoch 00016: val_loss did not improve\n",
      "323431/323431 [==============================] - 60s - loss: 0.3324 - target_output_loss: 0.3324 - feature_output_loss: 0.0000e+00 - val_loss: 0.3553 - val_target_output_loss: 0.3553 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 18/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3290 - target_output_loss: 0.3290 - feature_output_loss: 0.0000e+00Epoch 00017: val_loss did not improve\n",
      "323431/323431 [==============================] - 57s - loss: 0.3290 - target_output_loss: 0.3290 - feature_output_loss: 0.0000e+00 - val_loss: 0.3530 - val_target_output_loss: 0.3530 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 19/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3272 - target_output_loss: 0.3272 - feature_output_loss: 0.0000e+00Epoch 00018: val_loss did not improve\n",
      "323431/323431 [==============================] - 54s - loss: 0.3272 - target_output_loss: 0.3272 - feature_output_loss: 0.0000e+00 - val_loss: 0.3585 - val_target_output_loss: 0.3585 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 20/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3251 - target_output_loss: 0.3251 - feature_output_loss: 0.0000e+00Epoch 00019: val_loss did not improve\n",
      "323431/323431 [==============================] - 55s - loss: 0.3252 - target_output_loss: 0.3252 - feature_output_loss: 0.0000e+00 - val_loss: 0.3625 - val_target_output_loss: 0.3625 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 00019: early stopping\n",
      "80859/80859 [==============================] - 3s     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuriyguts/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:52: DeprecationWarning: assignment will raise an error in the future, most likely because your index result shape does not match the value array shape. You can use `arr.flat[index] = values` to keep the old behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting fold 2 of 5\n",
      "\n",
      "Train on 323431 samples, validate on 80859 samples\n",
      "Epoch 1/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.5440 - target_output_loss: 0.5440 - feature_output_loss: 0.0000e+00Epoch 00000: val_loss improved from inf to 0.48981, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.5440 - target_output_loss: 0.5440 - feature_output_loss: 0.0000e+00 - val_loss: 0.4898 - val_target_output_loss: 0.4898 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 2/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4811 - target_output_loss: 0.4811 - feature_output_loss: 0.0000e+00Epoch 00001: val_loss improved from 0.48981 to 0.46110, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.4811 - target_output_loss: 0.4811 - feature_output_loss: 0.0000e+00 - val_loss: 0.4611 - val_target_output_loss: 0.4611 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 3/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4495 - target_output_loss: 0.4495 - feature_output_loss: 0.0000e+00Epoch 00002: val_loss improved from 0.46110 to 0.42314, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.4495 - target_output_loss: 0.4495 - feature_output_loss: 0.0000e+00 - val_loss: 0.4231 - val_target_output_loss: 0.4231 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 4/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4296 - target_output_loss: 0.4296 - feature_output_loss: 0.0000e+00Epoch 00003: val_loss improved from 0.42314 to 0.40908, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 60s - loss: 0.4296 - target_output_loss: 0.4296 - feature_output_loss: 0.0000e+00 - val_loss: 0.4091 - val_target_output_loss: 0.4091 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 5/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4123 - target_output_loss: 0.4123 - feature_output_loss: 0.0000e+00Epoch 00004: val_loss improved from 0.40908 to 0.40113, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 57s - loss: 0.4123 - target_output_loss: 0.4123 - feature_output_loss: 0.0000e+00 - val_loss: 0.4011 - val_target_output_loss: 0.4011 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 6/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.4001 - target_output_loss: 0.4001 - feature_output_loss: 0.0000e+00Epoch 00005: val_loss improved from 0.40113 to 0.39418, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 61s - loss: 0.4002 - target_output_loss: 0.4002 - feature_output_loss: 0.0000e+00 - val_loss: 0.3942 - val_target_output_loss: 0.3942 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 7/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3892 - target_output_loss: 0.3892 - feature_output_loss: 0.0000e+00Epoch 00006: val_loss improved from 0.39418 to 0.37981, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 62s - loss: 0.3892 - target_output_loss: 0.3892 - feature_output_loss: 0.0000e+00 - val_loss: 0.3798 - val_target_output_loss: 0.3798 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 8/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3802 - target_output_loss: 0.3802 - feature_output_loss: 0.0000e+00Epoch 00007: val_loss improved from 0.37981 to 0.37511, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 61s - loss: 0.3802 - target_output_loss: 0.3802 - feature_output_loss: 0.0000e+00 - val_loss: 0.3751 - val_target_output_loss: 0.3751 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 9/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3726 - target_output_loss: 0.3726 - feature_output_loss: 0.0000e+00Epoch 00008: val_loss did not improve\n",
      "323431/323431 [==============================] - 61s - loss: 0.3725 - target_output_loss: 0.3725 - feature_output_loss: 0.0000e+00 - val_loss: 0.3798 - val_target_output_loss: 0.3798 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 10/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3648 - target_output_loss: 0.3648 - feature_output_loss: 0.0000e+00Epoch 00009: val_loss improved from 0.37511 to 0.37310, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 62s - loss: 0.3648 - target_output_loss: 0.3648 - feature_output_loss: 0.0000e+00 - val_loss: 0.3731 - val_target_output_loss: 0.3731 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 11/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3592 - target_output_loss: 0.3592 - feature_output_loss: 0.0000e+00Epoch 00010: val_loss improved from 0.37310 to 0.37240, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 59s - loss: 0.3593 - target_output_loss: 0.3593 - feature_output_loss: 0.0000e+00 - val_loss: 0.3724 - val_target_output_loss: 0.3724 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 12/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3527 - target_output_loss: 0.3527 - feature_output_loss: 0.0000e+00Epoch 00011: val_loss did not improve\n",
      "323431/323431 [==============================] - 62s - loss: 0.3527 - target_output_loss: 0.3527 - feature_output_loss: 0.0000e+00 - val_loss: 0.3823 - val_target_output_loss: 0.3823 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 13/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3497 - target_output_loss: 0.3497 - feature_output_loss: 0.0000e+00Epoch 00012: val_loss improved from 0.37240 to 0.36689, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 59s - loss: 0.3497 - target_output_loss: 0.3497 - feature_output_loss: 0.0000e+00 - val_loss: 0.3669 - val_target_output_loss: 0.3669 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 14/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3450 - target_output_loss: 0.3450 - feature_output_loss: 0.0000e+00Epoch 00013: val_loss improved from 0.36689 to 0.36526, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 59s - loss: 0.3451 - target_output_loss: 0.3451 - feature_output_loss: 0.0000e+00 - val_loss: 0.3653 - val_target_output_loss: 0.3653 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 15/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3407 - target_output_loss: 0.3407 - feature_output_loss: 0.0000e+00Epoch 00014: val_loss did not improve\n",
      "323431/323431 [==============================] - 60s - loss: 0.3407 - target_output_loss: 0.3407 - feature_output_loss: 0.0000e+00 - val_loss: 0.3666 - val_target_output_loss: 0.3666 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 16/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3371 - target_output_loss: 0.3371 - feature_output_loss: 0.0000e+00Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323431/323431 [==============================] - 57s - loss: 0.3371 - target_output_loss: 0.3371 - feature_output_loss: 0.0000e+00 - val_loss: 0.3687 - val_target_output_loss: 0.3687 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 17/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3334 - target_output_loss: 0.3334 - feature_output_loss: 0.0000e+00Epoch 00016: val_loss improved from 0.36526 to 0.35939, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 58s - loss: 0.3334 - target_output_loss: 0.3334 - feature_output_loss: 0.0000e+00 - val_loss: 0.3594 - val_target_output_loss: 0.3594 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 18/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3301 - target_output_loss: 0.3301 - feature_output_loss: 0.0000e+00Epoch 00017: val_loss did not improve\n",
      "323431/323431 [==============================] - 59s - loss: 0.3301 - target_output_loss: 0.3301 - feature_output_loss: 0.0000e+00 - val_loss: 0.3742 - val_target_output_loss: 0.3742 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 19/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3283 - target_output_loss: 0.3283 - feature_output_loss: 0.0000e+00Epoch 00018: val_loss did not improve\n",
      "323431/323431 [==============================] - 60s - loss: 0.3283 - target_output_loss: 0.3283 - feature_output_loss: 0.0000e+00 - val_loss: 0.3730 - val_target_output_loss: 0.3730 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 20/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3240 - target_output_loss: 0.3240 - feature_output_loss: 0.0000e+00Epoch 00019: val_loss improved from 0.35939 to 0.35915, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323431/323431 [==============================] - 59s - loss: 0.3240 - target_output_loss: 0.3240 - feature_output_loss: 0.0000e+00 - val_loss: 0.3592 - val_target_output_loss: 0.3592 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 21/200\n",
      "323072/323431 [============================>.] - ETA: 0s - loss: 0.3243 - target_output_loss: 0.3243 - feature_output_loss: 0.0000e+00Epoch 00020: val_loss did not improve\n",
      "323431/323431 [==============================] - 59s - loss: 0.3242 - target_output_loss: 0.3242 - feature_output_loss: 0.0000e+00 - val_loss: 0.3629 - val_target_output_loss: 0.3629 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 00020: early stopping\n",
      "80859/80859 [==============================] - 4s     \n",
      "\n",
      "Fitting fold 3 of 5\n",
      "\n",
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.5456 - target_output_loss: 0.5456 - feature_output_loss: 0.0000e+00Epoch 00000: val_loss improved from inf to 0.48623, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 62s - loss: 0.5455 - target_output_loss: 0.5455 - feature_output_loss: 0.0000e+00 - val_loss: 0.4862 - val_target_output_loss: 0.4862 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 2/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.4815 - target_output_loss: 0.4815 - feature_output_loss: 0.0000e+00Epoch 00001: val_loss improved from 0.48623 to 0.44827, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 60s - loss: 0.4815 - target_output_loss: 0.4815 - feature_output_loss: 0.0000e+00 - val_loss: 0.4483 - val_target_output_loss: 0.4483 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 3/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.4517 - target_output_loss: 0.4517 - feature_output_loss: 0.0000e+00Epoch 00002: val_loss improved from 0.44827 to 0.42468, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 60s - loss: 0.4517 - target_output_loss: 0.4517 - feature_output_loss: 0.0000e+00 - val_loss: 0.4247 - val_target_output_loss: 0.4247 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 4/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.4298 - target_output_loss: 0.4298 - feature_output_loss: 0.0000e+00Epoch 00003: val_loss improved from 0.42468 to 0.41000, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 58s - loss: 0.4298 - target_output_loss: 0.4298 - feature_output_loss: 0.0000e+00 - val_loss: 0.4100 - val_target_output_loss: 0.4100 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 5/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.4138 - target_output_loss: 0.4138 - feature_output_loss: 0.0000e+00Epoch 00004: val_loss improved from 0.41000 to 0.39988, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 58s - loss: 0.4138 - target_output_loss: 0.4138 - feature_output_loss: 0.0000e+00 - val_loss: 0.3999 - val_target_output_loss: 0.3999 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 6/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.4003 - target_output_loss: 0.4003 - feature_output_loss: 0.0000e+00Epoch 00005: val_loss improved from 0.39988 to 0.39423, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 57s - loss: 0.4002 - target_output_loss: 0.4002 - feature_output_loss: 0.0000e+00 - val_loss: 0.3942 - val_target_output_loss: 0.3942 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 7/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3883 - target_output_loss: 0.3883 - feature_output_loss: 0.0000e+00Epoch 00006: val_loss did not improve\n",
      "323432/323432 [==============================] - 59s - loss: 0.3883 - target_output_loss: 0.3883 - feature_output_loss: 0.0000e+00 - val_loss: 0.3955 - val_target_output_loss: 0.3955 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 8/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3790 - target_output_loss: 0.3790 - feature_output_loss: 0.0000e+00Epoch 00007: val_loss improved from 0.39423 to 0.38196, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 61s - loss: 0.3790 - target_output_loss: 0.3790 - feature_output_loss: 0.0000e+00 - val_loss: 0.3820 - val_target_output_loss: 0.3820 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 9/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3720 - target_output_loss: 0.3720 - feature_output_loss: 0.0000e+00Epoch 00008: val_loss improved from 0.38196 to 0.37770, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 61s - loss: 0.3720 - target_output_loss: 0.3720 - feature_output_loss: 0.0000e+00 - val_loss: 0.3777 - val_target_output_loss: 0.3777 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 10/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3644 - target_output_loss: 0.3644 - feature_output_loss: 0.0000e+00Epoch 00009: val_loss improved from 0.37770 to 0.36862, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 62s - loss: 0.3644 - target_output_loss: 0.3644 - feature_output_loss: 0.0000e+00 - val_loss: 0.3686 - val_target_output_loss: 0.3686 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3575 - target_output_loss: 0.3575 - feature_output_loss: 0.0000e+00Epoch 00010: val_loss did not improve\n",
      "323432/323432 [==============================] - 58s - loss: 0.3574 - target_output_loss: 0.3574 - feature_output_loss: 0.0000e+00 - val_loss: 0.3731 - val_target_output_loss: 0.3731 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 12/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3519 - target_output_loss: 0.3519 - feature_output_loss: 0.0000e+00Epoch 00011: val_loss did not improve\n",
      "323432/323432 [==============================] - 58s - loss: 0.3519 - target_output_loss: 0.3519 - feature_output_loss: 0.0000e+00 - val_loss: 0.3750 - val_target_output_loss: 0.3750 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 13/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3477 - target_output_loss: 0.3477 - feature_output_loss: 0.0000e+00Epoch 00012: val_loss did not improve\n",
      "323432/323432 [==============================] - 57s - loss: 0.3477 - target_output_loss: 0.3477 - feature_output_loss: 0.0000e+00 - val_loss: 0.3736 - val_target_output_loss: 0.3736 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 14/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3434 - target_output_loss: 0.3434 - feature_output_loss: 0.0000e+00Epoch 00013: val_loss improved from 0.36862 to 0.36393, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 56s - loss: 0.3434 - target_output_loss: 0.3434 - feature_output_loss: 0.0000e+00 - val_loss: 0.3639 - val_target_output_loss: 0.3639 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 15/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3392 - target_output_loss: 0.3392 - feature_output_loss: 0.0000e+00Epoch 00014: val_loss improved from 0.36393 to 0.35892, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 55s - loss: 0.3392 - target_output_loss: 0.3392 - feature_output_loss: 0.0000e+00 - val_loss: 0.3589 - val_target_output_loss: 0.3589 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 16/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3358 - target_output_loss: 0.3358 - feature_output_loss: 0.0000e+00Epoch 00015: val_loss did not improve\n",
      "323432/323432 [==============================] - 53s - loss: 0.3359 - target_output_loss: 0.3359 - feature_output_loss: 0.0000e+00 - val_loss: 0.3633 - val_target_output_loss: 0.3633 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 17/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3324 - target_output_loss: 0.3324 - feature_output_loss: 0.0000e+00Epoch 00016: val_loss did not improve\n",
      "323432/323432 [==============================] - 54s - loss: 0.3325 - target_output_loss: 0.3325 - feature_output_loss: 0.0000e+00 - val_loss: 0.3618 - val_target_output_loss: 0.3618 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 18/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3297 - target_output_loss: 0.3297 - feature_output_loss: 0.0000e+00Epoch 00017: val_loss improved from 0.35892 to 0.35551, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 57s - loss: 0.3297 - target_output_loss: 0.3297 - feature_output_loss: 0.0000e+00 - val_loss: 0.3555 - val_target_output_loss: 0.3555 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 19/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3262 - target_output_loss: 0.3262 - feature_output_loss: 0.0000e+00Epoch 00018: val_loss did not improve\n",
      "323432/323432 [==============================] - 65s - loss: 0.3262 - target_output_loss: 0.3262 - feature_output_loss: 0.0000e+00 - val_loss: 0.3595 - val_target_output_loss: 0.3595 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 20/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3257 - target_output_loss: 0.3257 - feature_output_loss: 0.0000e+00Epoch 00019: val_loss did not improve\n",
      "323432/323432 [==============================] - 66s - loss: 0.3257 - target_output_loss: 0.3257 - feature_output_loss: 0.0000e+00 - val_loss: 0.3599 - val_target_output_loss: 0.3599 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 21/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3223 - target_output_loss: 0.3223 - feature_output_loss: 0.0000e+00Epoch 00020: val_loss did not improve\n",
      "323432/323432 [==============================] - 65s - loss: 0.3223 - target_output_loss: 0.3223 - feature_output_loss: 0.0000e+00 - val_loss: 0.3593 - val_target_output_loss: 0.3593 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 22/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3210 - target_output_loss: 0.3210 - feature_output_loss: 0.0000e+00Epoch 00021: val_loss improved from 0.35551 to 0.35407, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 73s - loss: 0.3210 - target_output_loss: 0.3210 - feature_output_loss: 0.0000e+00 - val_loss: 0.3541 - val_target_output_loss: 0.3541 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 23/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3181 - target_output_loss: 0.3181 - feature_output_loss: 0.0000e+00Epoch 00022: val_loss did not improve\n",
      "323432/323432 [==============================] - 62s - loss: 0.3181 - target_output_loss: 0.3181 - feature_output_loss: 0.0000e+00 - val_loss: 0.3617 - val_target_output_loss: 0.3617 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 24/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3167 - target_output_loss: 0.3167 - feature_output_loss: 0.0000e+00Epoch 00023: val_loss did not improve\n",
      "323432/323432 [==============================] - 65s - loss: 0.3167 - target_output_loss: 0.3167 - feature_output_loss: 0.0000e+00 - val_loss: 0.3609 - val_target_output_loss: 0.3609 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 25/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3145 - target_output_loss: 0.3145 - feature_output_loss: 0.0000e+00Epoch 00024: val_loss improved from 0.35407 to 0.35346, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323432/323432 [==============================] - 65s - loss: 0.3146 - target_output_loss: 0.3146 - feature_output_loss: 0.0000e+00 - val_loss: 0.3535 - val_target_output_loss: 0.3535 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 26/200\n",
      "323072/323432 [============================>.] - ETA: 0s - loss: 0.3137 - target_output_loss: 0.3137 - feature_output_loss: 0.0000e+00Epoch 00025: val_loss did not improve\n",
      "323432/323432 [==============================] - 64s - loss: 0.3137 - target_output_loss: 0.3137 - feature_output_loss: 0.0000e+00 - val_loss: 0.3578 - val_target_output_loss: 0.3578 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 00025: early stopping\n",
      "79872/80858 [============================>.] - ETA: 0s\n",
      "Fitting fold 4 of 5\n",
      "\n",
      "Train on 323433 samples, validate on 80857 samples\n",
      "Epoch 1/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.5458 - target_output_loss: 0.5458 - feature_output_loss: 0.0000e+00Epoch 00000: val_loss improved from inf to 0.48471, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 68s - loss: 0.5457 - target_output_loss: 0.5457 - feature_output_loss: 0.0000e+00 - val_loss: 0.4847 - val_target_output_loss: 0.4847 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 2/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.4830 - target_output_loss: 0.4830 - feature_output_loss: 0.0000e+00Epoch 00001: val_loss improved from 0.48471 to 0.44295, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323433/323433 [==============================] - 53s - loss: 0.4830 - target_output_loss: 0.4830 - feature_output_loss: 0.0000e+00 - val_loss: 0.4429 - val_target_output_loss: 0.4429 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 3/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.4534 - target_output_loss: 0.4534 - feature_output_loss: 0.0000e+00Epoch 00002: val_loss improved from 0.44295 to 0.42078, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 53s - loss: 0.4534 - target_output_loss: 0.4534 - feature_output_loss: 0.0000e+00 - val_loss: 0.4208 - val_target_output_loss: 0.4208 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 4/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.4323 - target_output_loss: 0.4323 - feature_output_loss: 0.0000e+00Epoch 00003: val_loss improved from 0.42078 to 0.41409, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 56s - loss: 0.4323 - target_output_loss: 0.4323 - feature_output_loss: 0.0000e+00 - val_loss: 0.4141 - val_target_output_loss: 0.4141 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 5/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.4158 - target_output_loss: 0.4158 - feature_output_loss: 0.0000e+00Epoch 00004: val_loss improved from 0.41409 to 0.40028, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 58s - loss: 0.4158 - target_output_loss: 0.4158 - feature_output_loss: 0.0000e+00 - val_loss: 0.4003 - val_target_output_loss: 0.4003 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 6/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.4028 - target_output_loss: 0.4028 - feature_output_loss: 0.0000e+00Epoch 00005: val_loss improved from 0.40028 to 0.38791, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 58s - loss: 0.4028 - target_output_loss: 0.4028 - feature_output_loss: 0.0000e+00 - val_loss: 0.3879 - val_target_output_loss: 0.3879 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 7/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3919 - target_output_loss: 0.3919 - feature_output_loss: 0.0000e+00Epoch 00006: val_loss improved from 0.38791 to 0.38472, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 55s - loss: 0.3919 - target_output_loss: 0.3919 - feature_output_loss: 0.0000e+00 - val_loss: 0.3847 - val_target_output_loss: 0.3847 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 8/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3813 - target_output_loss: 0.3813 - feature_output_loss: 0.0000e+00Epoch 00007: val_loss improved from 0.38472 to 0.37898, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 54s - loss: 0.3813 - target_output_loss: 0.3813 - feature_output_loss: 0.0000e+00 - val_loss: 0.3790 - val_target_output_loss: 0.3790 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 9/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3741 - target_output_loss: 0.3741 - feature_output_loss: 0.0000e+00Epoch 00008: val_loss did not improve\n",
      "323433/323433 [==============================] - 61s - loss: 0.3741 - target_output_loss: 0.3741 - feature_output_loss: 0.0000e+00 - val_loss: 0.3813 - val_target_output_loss: 0.3813 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 10/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3674 - target_output_loss: 0.3674 - feature_output_loss: 0.0000e+00Epoch 00009: val_loss improved from 0.37898 to 0.37303, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 64s - loss: 0.3675 - target_output_loss: 0.3675 - feature_output_loss: 0.0000e+00 - val_loss: 0.3730 - val_target_output_loss: 0.3730 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 11/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3605 - target_output_loss: 0.3605 - feature_output_loss: 0.0000e+00Epoch 00010: val_loss improved from 0.37303 to 0.37016, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 64s - loss: 0.3605 - target_output_loss: 0.3605 - feature_output_loss: 0.0000e+00 - val_loss: 0.3702 - val_target_output_loss: 0.3702 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 12/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3561 - target_output_loss: 0.3561 - feature_output_loss: 0.0000e+00Epoch 00011: val_loss did not improve\n",
      "323433/323433 [==============================] - 67s - loss: 0.3561 - target_output_loss: 0.3561 - feature_output_loss: 0.0000e+00 - val_loss: 0.3774 - val_target_output_loss: 0.3774 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 13/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3509 - target_output_loss: 0.3509 - feature_output_loss: 0.0000e+00Epoch 00012: val_loss improved from 0.37016 to 0.36163, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 70s - loss: 0.3510 - target_output_loss: 0.3510 - feature_output_loss: 0.0000e+00 - val_loss: 0.3616 - val_target_output_loss: 0.3616 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 14/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3463 - target_output_loss: 0.3463 - feature_output_loss: 0.0000e+00Epoch 00013: val_loss did not improve\n",
      "323433/323433 [==============================] - 65s - loss: 0.3463 - target_output_loss: 0.3463 - feature_output_loss: 0.0000e+00 - val_loss: 0.3644 - val_target_output_loss: 0.3644 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 15/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3421 - target_output_loss: 0.3421 - feature_output_loss: 0.0000e+00Epoch 00014: val_loss did not improve\n",
      "323433/323433 [==============================] - 67s - loss: 0.3421 - target_output_loss: 0.3421 - feature_output_loss: 0.0000e+00 - val_loss: 0.3699 - val_target_output_loss: 0.3699 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 16/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3387 - target_output_loss: 0.3387 - feature_output_loss: 0.0000e+00Epoch 00015: val_loss did not improve\n",
      "323433/323433 [==============================] - 64s - loss: 0.3388 - target_output_loss: 0.3388 - feature_output_loss: 0.0000e+00 - val_loss: 0.3651 - val_target_output_loss: 0.3651 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 17/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3352 - target_output_loss: 0.3352 - feature_output_loss: 0.0000e+00Epoch 00016: val_loss improved from 0.36163 to 0.35977, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 66s - loss: 0.3352 - target_output_loss: 0.3352 - feature_output_loss: 0.0000e+00 - val_loss: 0.3598 - val_target_output_loss: 0.3598 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 18/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3327 - target_output_loss: 0.3327 - feature_output_loss: 0.0000e+00Epoch 00017: val_loss improved from 0.35977 to 0.35679, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323433/323433 [==============================] - 59s - loss: 0.3327 - target_output_loss: 0.3327 - feature_output_loss: 0.0000e+00 - val_loss: 0.3568 - val_target_output_loss: 0.3568 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 19/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3301 - target_output_loss: 0.3301 - feature_output_loss: 0.0000e+00Epoch 00018: val_loss improved from 0.35679 to 0.35454, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 59s - loss: 0.3302 - target_output_loss: 0.3302 - feature_output_loss: 0.0000e+00 - val_loss: 0.3545 - val_target_output_loss: 0.3545 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 20/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3273 - target_output_loss: 0.3273 - feature_output_loss: 0.0000e+00Epoch 00019: val_loss did not improve\n",
      "323433/323433 [==============================] - 60s - loss: 0.3274 - target_output_loss: 0.3274 - feature_output_loss: 0.0000e+00 - val_loss: 0.3649 - val_target_output_loss: 0.3649 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 21/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3251 - target_output_loss: 0.3251 - feature_output_loss: 0.0000e+00Epoch 00020: val_loss did not improve\n",
      "323433/323433 [==============================] - 58s - loss: 0.3251 - target_output_loss: 0.3251 - feature_output_loss: 0.0000e+00 - val_loss: 0.3572 - val_target_output_loss: 0.3572 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 22/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3231 - target_output_loss: 0.3231 - feature_output_loss: 0.0000e+00Epoch 00021: val_loss did not improve\n",
      "323433/323433 [==============================] - 55s - loss: 0.3231 - target_output_loss: 0.3231 - feature_output_loss: 0.0000e+00 - val_loss: 0.3644 - val_target_output_loss: 0.3644 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 23/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3208 - target_output_loss: 0.3208 - feature_output_loss: 0.0000e+00Epoch 00022: val_loss improved from 0.35454 to 0.35329, saving model to /home/yuriyguts/Projects/kaggle-quora-question-pairs/data/aux/fold-checkpoint-oofp_siamese_lstm_attention_activations.h5\n",
      "323433/323433 [==============================] - 61s - loss: 0.3208 - target_output_loss: 0.3208 - feature_output_loss: 0.0000e+00 - val_loss: 0.3533 - val_target_output_loss: 0.3533 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 24/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3192 - target_output_loss: 0.3192 - feature_output_loss: 0.0000e+00Epoch 00023: val_loss did not improve\n",
      "323433/323433 [==============================] - 60s - loss: 0.3192 - target_output_loss: 0.3192 - feature_output_loss: 0.0000e+00 - val_loss: 0.3540 - val_target_output_loss: 0.3540 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 25/200\n",
      "323072/323433 [============================>.] - ETA: 0s - loss: 0.3171 - target_output_loss: 0.3171 - feature_output_loss: 0.0000e+00Epoch 00024: val_loss did not improve\n",
      "323433/323433 [==============================] - 59s - loss: 0.3171 - target_output_loss: 0.3171 - feature_output_loss: 0.0000e+00 - val_loss: 0.3675 - val_target_output_loss: 0.3675 - val_feature_output_loss: 0.0000e+00\n",
      "Epoch 26/200\n",
      "248320/323433 [======================>.......] - ETA: 11s - loss: 0.3160 - target_output_loss: 0.3160 - feature_output_loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_q1, y_train)):\n",
    "    X_fold_train_q1 = X_train_q1[ix_train]\n",
    "    X_fold_train_q2 = X_train_q2[ix_train]\n",
    "\n",
    "    X_fold_val_q1 = X_train_q1[ix_val]\n",
    "    X_fold_val_q2 = X_train_q2[ix_val]\n",
    "\n",
    "    y_fold_train = y_train[ix_train]\n",
    "    y_fold_val = y_train[ix_val]\n",
    "    \n",
    "    print()\n",
    "    print(f'Fitting fold {fold_num + 1} of {kfold.n_splits}')\n",
    "    print()\n",
    "    \n",
    "    model = create_model(model_params)\n",
    "    model.fit(\n",
    "        [X_fold_train_q1, X_fold_train_q2],\n",
    "        [y_fold_train, np.zeros((len(y_fold_train), feature_output_size))],\n",
    "        \n",
    "        validation_data=(\n",
    "            [X_fold_val_q1, X_fold_val_q2],\n",
    "            [y_fold_val, np.zeros((len(y_fold_val), feature_output_size))],\n",
    "        ),\n",
    "\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=1,\n",
    "        \n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.001,\n",
    "                patience=3,\n",
    "                verbose=1,\n",
    "                mode='auto',\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_checkpoint_path,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=2,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "        \n",
    "    # Create out-of-fold prediction.\n",
    "    model.load_weights(model_checkpoint_path)\n",
    "    \n",
    "    y_train_oofp[ix_val], y_train_oofp_features[ix_val] = model.predict(\n",
    "        [X_train_q1[ix_val], X_train_q2[ix_val]],\n",
    "        batch_size=1024,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    if fold_num + 1 == NUM_FOLDS:\n",
    "        y_test_oofp, y_test_oofp_features = model.predict(\n",
    "            [X_test_q1, X_test_q2],\n",
    "            batch_size=1024,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    # Clear GPU memory.\n",
    "    K.clear_session()\n",
    "    del X_fold_train_q1\n",
    "    del X_fold_train_q2\n",
    "    del X_fold_val_q1\n",
    "    del X_fold_val_q2\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = log_loss(y_train, y_train_oofp)\n",
    "print('CV score:', cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'oofp_siamese_lstm_attention_activations',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_feature_names(feature_names, feature_list_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_feature_list(y_train_oofp_features, 'train', feature_list_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_feature_list(y_test_oofp_features, 'test', feature_list_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_features).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_features).describe().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
